{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1257215,"sourceType":"datasetVersion","datasetId":723100}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Standard Libraries\nimport re\nimport statistics as stats\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# NLP Libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Deep Learning and Data Handling\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\n\n# Data Augmentation (will be used later)\n# We can use nlpaug, but for now, let's focus on a simple synonym replacement as an example.\nfrom nltk.corpus import wordnet\n\n# Check for GPU\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n# Download necessary NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\n\n# Define device\ndevice = 'cuda' if len(tf.config.experimental.list_physical_devices('GPU')) > 0 else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:24.018761Z","iopub.execute_input":"2025-09-16T11:31:24.019022Z","iopub.status.idle":"2025-09-16T11:31:38.877551Z","shell.execute_reply.started":"2025-09-16T11:31:24.018999Z","shell.execute_reply":"2025-09-16T11:31:38.876831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset input","metadata":{}},{"cell_type":"code","source":"# Load the dataset\nfile_path = \"/kaggle/input/hate-speech-and-offensive-language-dataset/labeled_data.csv\"\ndata = pd.read_csv(file_path)\n\n# Display initial information\nprint(\"Dataset Head:\")\nprint(data.head())\nprint(\"\\nDataset Info:\")\nprint(data.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:38.87879Z","iopub.execute_input":"2025-09-16T11:31:38.879196Z","iopub.status.idle":"2025-09-16T11:31:38.99626Z","shell.execute_reply.started":"2025-09-16T11:31:38.879179Z","shell.execute_reply":"2025-09-16T11:31:38.995647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data pre-processing","metadata":{}},{"cell_type":"code","source":"# Drop unnecessary column\ndata_cleaned = data.drop(columns=['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'])\n\n# Rename 'class' column for clarity\ndata_cleaned.rename(columns={'class': 'label'}, inplace=True)\n\n# Map numeric labels to descriptive labels\nlabel_mapping = {0: 'Hate Speech', 1: 'Offensive Language', 2: 'Neither'}\ndata_cleaned['label_desc'] = data_cleaned['label'].map(label_mapping)\n\nprint(\"\\nCleaned Data Head:\")\nprint(data_cleaned.head())\nprint(\"\\nValue Counts for labels:\")\nprint(data_cleaned['label_desc'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:38.99692Z","iopub.execute_input":"2025-09-16T11:31:38.997118Z","iopub.status.idle":"2025-09-16T11:31:39.009299Z","shell.execute_reply.started":"2025-09-16T11:31:38.997102Z","shell.execute_reply":"2025-09-16T11:31:39.008592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize class distribution\nplt.figure(figsize=(8, 6))\ndata_cleaned['label_desc'].value_counts().plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])\nplt.title('Class Distribution')\nplt.xlabel('Class')\nplt.ylabel('Number of Samples')\nplt.xticks(rotation=0)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:39.009995Z","iopub.execute_input":"2025-09-16T11:31:39.010229Z","iopub.status.idle":"2025-09-16T11:31:39.531313Z","shell.execute_reply.started":"2025-09-16T11:31:39.010204Z","shell.execute_reply":"2025-09-16T11:31:39.530553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    # Remove mentions\n    text = re.sub(r'@\\w+', '', text)\n    # Remove special characters, numbers and punctuation, but keep spaces\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Convert to lowercase\n    text = text.lower()\n    # Tokenize and remove stopwords\n    tokens = word_tokenize(text)\n    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n    # Join tokens back into a string\n    return \" \".join(filtered_tokens)\n\n# Apply cleaning function to the tweet column\ndata_cleaned['tweet'] = data_cleaned['tweet'].apply(clean_text)\n\nprint(\"\\nData after cleaning:\")\nprint(data_cleaned.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:39.53303Z","iopub.execute_input":"2025-09-16T11:31:39.533252Z","iopub.status.idle":"2025-09-16T11:31:41.33926Z","shell.execute_reply.started":"2025-09-16T11:31:39.533233Z","shell.execute_reply":"2025-09-16T11:31:41.338623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_synonyms(word):\n    \"\"\"Get a list of synonyms for a given word.\"\"\"\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonym = lemma.name().replace(\"_\", \" \").lower()\n            if synonym != word:\n                synonyms.add(synonym)\n    return list(synonyms)\n\ndef augment_data(text, num_augmentations=1):\n    \"\"\"\n    Augments a text by randomly replacing a word with a synonym.\n    \n    Args:\n        text (str): The input text.\n        num_augmentations (int): The number of new augmented samples to create.\n        \n    Returns:\n        list: A list of augmented texts.\n    \"\"\"\n    words = text.split()\n    augmented_texts = []\n    \n    for _ in range(num_augmentations):\n        new_words = list(words)\n        \n        # Choose a random word to replace\n        if not new_words:\n            continue\n            \n        random_word_idx = np.random.randint(0, len(new_words))\n        random_word = new_words[random_word_idx]\n        \n        # Get synonyms and replace if any are found\n        synonyms = get_synonyms(random_word)\n        if synonyms:\n            random_synonym = np.random.choice(synonyms)\n            new_words[random_word_idx] = random_synonym\n        \n        augmented_texts.append(\" \".join(new_words))\n        \n    return augmented_texts\n\n# Separate the minority class\nhate_speech_data = data_cleaned[data_cleaned['label'] == 0].copy()\n\n# Augment the hate speech data\naugmented_samples = []\nfor index, row in hate_speech_data.iterrows():\n    augmented_texts = augment_data(row['tweet'], num_augmentations=3) # Create 3 new samples per original text\n    for aug_text in augmented_texts:\n        augmented_samples.append({'tweet': aug_text, 'label': 0, 'label_desc': 'Hate Speech'})\n\n# Convert augmented samples to a DataFrame and concatenate\naugmented_df = pd.DataFrame(augmented_samples)\ndata_augmented = pd.concat([data_cleaned, augmented_df], ignore_index=True)\n\n# Check the new class distribution\nprint(\"\\nClass Distribution after Augmentation:\")\nprint(data_augmented['label_desc'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:41.339935Z","iopub.execute_input":"2025-09-16T11:31:41.340127Z","iopub.status.idle":"2025-09-16T11:31:44.701059Z","shell.execute_reply.started":"2025-09-16T11:31:41.340112Z","shell.execute_reply":"2025-09-16T11:31:44.700328Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Balancing the dataset","metadata":{}},{"cell_type":"code","source":"# Separate the dataframe into classes\ndf_hate_speech = data_augmented[data_augmented['label'] == 0].copy()\ndf_offensive_lang = data_augmented[data_augmented['label'] == 1].copy()\ndf_neither = data_augmented[data_augmented['label'] == 2].copy()\n\n# Determine the target count (size of the majority class)\ntarget_count = len(df_offensive_lang)\n\nprint(f\"Target count for each class: {target_count}\")\n\n# Function to augment a class to a target count\ndef augment_to_target(df_class, target):\n    if len(df_class) >= target:\n        return df_class.sample(n=target, replace=False, random_state=42)\n    \n    current_count = len(df_class)\n    augmented_samples = []\n\n    # Calculate how many new samples are needed\n    samples_to_add = target - current_count\n    \n    # Randomly select samples from the current class to augment\n    samples_to_augment = df_class.sample(n=samples_to_add, replace=True, random_state=42)\n\n    for index, row in samples_to_augment.iterrows():\n        # Using our previously defined augment_data function\n        new_text = augment_data(row['tweet'], num_augmentations=1)[0]\n        augmented_samples.append({\n            'tweet': new_text,\n            'label': row['label'],\n            'label_desc': row['label_desc']\n        })\n\n    # Concatenate the original and augmented data\n    augmented_df = pd.DataFrame(augmented_samples)\n    return pd.concat([df_class, augmented_df], ignore_index=True)\n\n# Augment the \"Hate Speech\" and \"Neither\" classes\nbalanced_hate_speech = augment_to_target(df_hate_speech, target_count)\nbalanced_neither = augment_to_target(df_neither, target_count)\n\n# Combine all the classes into a single balanced dataset\ndata_balanced = pd.concat([df_offensive_lang, balanced_hate_speech, balanced_neither], ignore_index=True)\n\n# Shuffle the dataframe to ensure the data is randomly distributed\ndata_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"\\nFinal Class Distribution after Strategic Augmentation:\")\nprint(data_balanced['label_desc'].value_counts())\n\n# Visualize the balanced class distribution\nplt.figure(figsize=(8, 6))\ndata_balanced['label_desc'].value_counts().plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])\nplt.title('Class Distribution after Strategic Oversampling')\nplt.xlabel('Class')\nplt.ylabel('Number of Samples')\nplt.xticks(rotation=0)\nplt.show()\n\n# Final check of the dataset size\nprint(f\"\\nTotal samples in balanced dataset: {len(data_balanced)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:44.701873Z","iopub.execute_input":"2025-09-16T11:31:44.7025Z","iopub.status.idle":"2025-09-16T11:31:48.872501Z","shell.execute_reply.started":"2025-09-16T11:31:44.702476Z","shell.execute_reply":"2025-09-16T11:31:48.871733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"code","source":"# Install the transformers library if you haven't already\n!pip install transformers\n\n# Import necessary libraries\nfrom transformers import BertTokenizer, TFBertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nimport numpy as np\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Split the balanced dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data_balanced['tweet'],\n    data_balanced['label'],\n    test_size=0.2,\n    random_state=42,\n    stratify=data_balanced['label']  # Stratify to maintain class distribution in splits\n)\n\n# One-hot encode the labels\ny_train_encoded = to_categorical(y_train, num_classes=3)\ny_test_encoded = to_categorical(y_test, num_classes=3)\nmax_seq_len=128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:31:48.873207Z","iopub.execute_input":"2025-09-16T11:31:48.873391Z","iopub.status.idle":"2025-09-16T11:32:01.958273Z","shell.execute_reply.started":"2025-09-16T11:31:48.873375Z","shell.execute_reply":"2025-09-16T11:32:01.95772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset spliting","metadata":{}},{"cell_type":"code","source":"def tokenize_data(texts, tokenizer, max_len=128):\n    input_ids = []\n    attention_masks = []\n\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,      # Add '[CLS]' and '[SEP]'\n            max_length=max_len,           # Pad/truncate to this length\n            padding='max_length',         # Pad to max_length\n            return_attention_mask=True,   # Return attention mask\n            return_tensors='tf',          # Return TensorFlow tensors\n            truncation=True               # Truncate sequences longer than max_length\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    input_ids = tf.concat(input_ids, axis=0)\n    attention_masks = tf.concat(attention_masks, axis=0)\n\n    return input_ids, attention_masks\n\n# Tokenize training and test sets\nX_train_input_ids, X_train_attention_masks = tokenize_data(X_train.tolist(), tokenizer, max_seq_len)\nX_test_input_ids, X_test_attention_masks = tokenize_data(X_test.tolist(), tokenizer, max_seq_len)\n\n# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    ({'input_ids': X_train_input_ids, 'attention_mask': X_train_attention_masks}, y_train_encoded)\n).shuffle(100).batch(32)\n\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    ({'input_ids': X_test_input_ids, 'attention_mask': X_test_attention_masks}, y_test_encoded)\n).batch(32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:32:01.959201Z","iopub.execute_input":"2025-09-16T11:32:01.960021Z","iopub.status.idle":"2025-09-16T11:32:41.492597Z","shell.execute_reply.started":"2025-09-16T11:32:01.959996Z","shell.execute_reply":"2025-09-16T11:32:41.49182Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load BERT model","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\nfrom torch.optim import AdamW\n\n# Load PyTorch BERT model\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=3\n)\n\n# Optimizer (AdamW is the recommended optimizer for BERT)\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\nprint(\"BERT PyTorch model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:32:41.493435Z","iopub.execute_input":"2025-09-16T11:32:41.49368Z","iopub.status.idle":"2025-09-16T11:32:47.898696Z","shell.execute_reply.started":"2025-09-16T11:32:41.493662Z","shell.execute_reply":"2025-09-16T11:32:47.898116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Function to tokenize and prepare data for BERT\ndef prepare_data(texts, labels, tokenizer, max_len=128):\n    input_ids = []\n    attention_masks = []\n    \n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels.values)\n\n    return input_ids, attention_masks, labels\n\n# Assuming X_train, y_train, X_test, y_test are already available from your previous steps\n# The data must be in pandas Series or numpy arrays for this to work\ntrain_input_ids, train_attention_masks, train_labels = prepare_data(X_train, y_train, tokenizer)\ntest_input_ids, test_attention_masks, test_labels = prepare_data(X_test, y_test, tokenizer)\n\n# Create the PyTorch datasets\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n\n# Create DataLoaders for batching and shuffling\nbatch_size = 32\ntrain_dataloader = DataLoader(\n    train_dataset,\n    sampler=RandomSampler(train_dataset), # Shuffles the data during training\n    batch_size=batch_size\n)\nvalidation_dataloader = DataLoader(\n    test_dataset,\n    sampler=SequentialSampler(test_dataset), # Does not shuffle the data for validation\n    batch_size=batch_size\n)\n\nprint(\"DataLoaders prepared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:32:47.899387Z","iopub.execute_input":"2025-09-16T11:32:47.899841Z","iopub.status.idle":"2025-09-16T11:33:11.192377Z","shell.execute_reply.started":"2025-09-16T11:32:47.899823Z","shell.execute_reply":"2025-09-16T11:33:11.191694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.notebook import tqdm\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint(f\"Using device: {device}\")\n\n# Training parameters\nepochs = 10   # Increased epochs since early stopping will stop earlier if needed\npatience = 3  # Number of epochs with no improvement after which training will stop\nbest_val_loss = float(\"inf\")\npatience_counter = 0\n\ntotal_steps = len(train_dataloader) * epochs\noptimizer = AdamW(model.parameters(), lr=3e-5)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\nloss_fn = CrossEntropyLoss()\n\n# Trackers for plotting\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\n# Training loop with Early Stopping\nfor epoch_i in range(epochs):\n    print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n    print(\"Training...\")\n\n    model.train()\n    total_train_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs.loss\n        total_train_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n\n        if step % 50 == 0 and step != 0:\n            avg_train_loss = total_train_loss / (step + 1)\n            print(f\"  Batch {step} of {len(train_dataloader)}. Loss: {avg_train_loss:.4f}\")\n\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    train_losses.append(avg_train_loss)\n    print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n\n    # Validation\n    print(\"\\nRunning Validation...\")\n    model.eval()\n\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in validation_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        with torch.no_grad():\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n            loss = outputs.loss\n            logits = outputs.logits\n\n        total_eval_loss += loss.item()\n        \n        preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n        labels = b_labels.cpu().numpy()\n        \n        all_preds.extend(preds)\n        all_labels.extend(labels)\n        \n        accuracy = np.sum(preds == labels) / len(labels)\n        total_eval_accuracy += accuracy\n\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(avg_val_accuracy)\n\n    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"  Validation Accuracy: {avg_val_accuracy:.4f}\")\n\n    # ---- Early Stopping Logic ----\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")  # Save best model\n        print(\"  Validation loss improved, model saved!\")\n    else:\n        patience_counter += 1\n        print(f\"  No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n        if patience_counter >= patience:\n            print(\"\\nEarly stopping triggered! Training stopped.\")\n            break\n\nprint(\"\\nTraining complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T11:33:11.193223Z","iopub.execute_input":"2025-09-16T11:33:11.193463Z","iopub.status.idle":"2025-09-16T12:31:50.568282Z","shell.execute_reply.started":"2025-09-16T11:33:11.193447Z","shell.execute_reply":"2025-09-16T12:31:50.567624Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Report generation","metadata":{}},{"cell_type":"code","source":"# Classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(all_labels, all_preds))\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=np.unique(all_labels), \n            yticklabels=np.unique(all_labels))\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Plot Loss & Accuracy curves\nepochs_range = range(1, len(train_losses) + 1)  # Use actual completed epochs\n\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(epochs_range, train_losses, label='Training Loss')\nplt.plot(epochs_range, val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epochs_range, val_accuracies, label='Validation Accuracy', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Validation Accuracy')\nplt.legend()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T12:31:50.56903Z","iopub.execute_input":"2025-09-16T12:31:50.569727Z","iopub.status.idle":"2025-09-16T12:31:51.06623Z","shell.execute_reply.started":"2025-09-16T12:31:50.5697Z","shell.execute_reply":"2025-09-16T12:31:51.065529Z"}},"outputs":[],"execution_count":null}]}